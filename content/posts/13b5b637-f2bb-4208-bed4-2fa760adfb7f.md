---
title: 谷歌人工智能搜索工具告诉用户“吃石头”对健康有益
date: 2024-05-24T17:06:52.047Z
description: ‘Hallucinations’ in new feature provoke ridicule and raise questions about betting core business on an experimental product
tag: 

- Tag companies
author: ft
---

[原文链接](https://ft.com/content/13b5b637-f2bb-4208-bed4-2fa760adfb7f)

谷歌人工智能搜索工具告诉用户“吃石头”对健康有益

## 摘要：
谷歌的 AI 搜索工具 AI Overviews 向用户提供了错误信息，例如推荐吃石头有健康益处或将奶酪粘在披萨上。这些“错觉”引起了人们对谷歌核心产品的可靠性及其将实验功能嵌入其中的决定的担忧。

谷歌承认，大多数 AI Overviews 提供高质量的信息，但有些查询不常见、经过篡改或无法复制。该公司正在根据内容政策迅速采取行动，并利用这些实例进行更广泛的系统改进。然而，“错觉”的出现仍然是消费者和大宗商品应用中的一个重大担忧。

## 有趣的问题：

1. 问题：谷歌的 AI Overviews 功能如何工作，其预期收益是什么？
   答案：谷歌的 AI Overviews 是一个实验性功能，它使用 Gemini 模型为用户查询提供简短答案。这个功能背后的意图是通过在常见搜索结果顶部提供快速、相关的信息来增强用户的搜索体验。通过利用生成式 AI，谷歌旨在为其数十亿用户简化寻找准确、可核实信息的过程。

2. 问题：在 AI 生成的内容中，“错觉”是指什么，为什么它们会带来挑战？
   答案：“错觉”是指不基于事实数据的捏造信息。这些预测系统通过选择序列中的可能下一个单词来工作，这有时会导致不正确或误导性内容。错觉带来挑战，因为它们破坏了用户对 AI 生成的答案所信任，并可能导致传播错误信息。

3. 问题：谷歌如何解决人们对 AI Overviews 功能可靠性的担忧，正在采取什么措施？
   答案：针对其 AI Overviews 功能的错误输出所引起的担忧，谷歌承认大多数例子提供了高质量的信息，但也认识到发生过捏造的情况。该公司根据内容政策迅速采取行动，对不适当或误导性响应采取行动，并利用这些案例进行更广泛的系统改进以减少错觉的发生。此外，谷歌首席执行官孙达尔·皮查伊（Sundar Pichai）强调了维护与用户信任的重要性，确保在人们依赖搜索结果的关键时刻提供准确信息。

---

## Summary:
Google's AI search tool, **AI Overviews**, has been providing users with erroneous information such as recommending eating rocks for health benefits or gluing cheese to pizza. These "hallucinations" have raised concerns about the reliability of Google's core product and its decision to embed experimental features into it.

Google acknowledges that most AI Overviews provide high-quality information, but some examples were uncommon queries, doctored, or could not be reproduced. The company is taking swift action under content policies and using these instances for broader system improvements. However, the occurrence of "hallucinations" remains a significant concern in consumer and business applications.

## Interesting Questions:
1. Q: How does Google's AI Overviews feature work, and what are its intended benefits?
A: Google's AI Overviews is an experimental feature that provides brief answers to user queries using the **Gemini** model. The intention behind this feature is to enhance users' search experience by offering quick and relevant information at the top of common search results. By leveraging generative AI, Google aims to streamline the process of finding accurate and verifiable information for its billions of users.

2. Q: What are "hallucinations" in the context of AI-generated content, and why do they pose challenges?
A: In the realm of generative AI models like Google's Gemini or Microsoft's partnership with OpenAI, "hallucinations" refer to fabricated information that is not based on factual data. These predictive systems work by choosing likely next words in a sequence, which can sometimes result in incorrect or misleading content. Hallucinations pose challenges because they undermine the trust users have in AI-generated answers and may lead to the dissemination of false information.

3. Q: How is Google addressing concerns about the reliability of its AI Overviews feature, and what measures are being taken?
A: In response to concerns raised by erroneous outputs from their AI Overviews feature, Google has acknowledged that most examples provide high-quality information but also recognized instances where fabrications occurred. The company is taking swift action under content policies for inappropriate or misleading responses and using these cases as a basis for broader system improvements to reduce the occurrence of hallucinations. Additionally, Google's CEO Sundar Pichai has emphasized the importance of maintaining trust with users by ensuring accurate information is provided during critical moments when people rely on search results.

[Source Link](https://ft.com/content/13b5b637-f2bb-4208-bed4-2fa760adfb7f)

