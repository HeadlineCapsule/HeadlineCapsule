---
title: 黑客们“越狱”强大的AI模型，以全球共同努力来凸显缺陷
date: 2024-06-21T04:14:46.554Z
description: Experts join forces in search for vulnerabilities in large language models made by OpenAI, Google and Elon Musk’s xAI
tags: 
- technology
author: ft
---

[原文链接](https://ft.com/content/14a2c98b-c8d5-4e5b-a7b0-30f0a05ec432)

黑客们“越狱”强大的AI模型，以全球共同努力来凸显缺陷

# 文章摘要

这篇文章讨论了黑客和研究人员在全球范围内努力发现大型语言模型（LLM）中的漏洞，例如 OpenAI 的 GPT-4、Google 的 Gemini 和 Meta 的 Llama 3。这些“越狱”尝试旨在揭露这些 AI 系统中的缺陷，而这些系统正被科技公司迅速发布以谋取利益。

## 要点：

1. 黑客如 Pliny the Prompter 操纵强大的 AI 模型，暴露它们的弱点并促进关于改进安全措施的讨论。
2. 在 LLM 中发现漏洞导致专注于机器学习安全性的初创企业市场增长，投资在一年内从 7000 万美元增加到 2.13 亿美元。
3. 世界各地的监管机构正在采取措施应对 AI 模型的潜在危险，包括欧盟的《人工智能法案》和加利福尼亚州提出的法案，要求公司披露黑客使用 LLM 造成的数据泄露。
4. 黑客使用各种技术来越狱 LLM，从简单的同义词替换到更复杂的攻击，利用模型的上下文窗口。
5. 尽管 Meta 和 OpenAI 等公司保证他们正在不断努力改进安全措施，但专家们警告说，随着 AI 系统与现有技术和设备越来越互联，风险也在增加。

## 结论：

大型语言模型的快速发展和部署导致了黑客和研究人员在全球范围内努力发现这些系统中的漏洞。随着世界各地的监管机构采取措施应对潜在的危险，公司继续投资于机器学习安全初创企业，同时致力于改进其 AI 系统的防御能力，以抵抗利用和对抗行为。

---

 **Article Summary**

The article discusses the global effort by hackers and researchers to identify vulnerabilities in large language models (LLMs) such as OpenAI's GPT-4, Google's Gemini, and Meta's Llama 3. These "jailbreak" attempts aim to highlight flaws within these AI systems that are being rapidly released by tech companies for profit.

**Key Points:**

1. Hackers like Pliny the Prompter manipulate powerful AI models, exposing their weaknesses and prompting discussions on improving security measures.
2. The vulnerabilities found in LLMs have led to a growing market for startups focused on machine learning security, with investments increasing from $70 million to $213 million within a year.
3. Regulators worldwide are taking steps to address potential dangers associated with AI models, including the EU's AI Act and California's proposed bill requiring companies to disclose data breaches caused by hackers using LLMs.
4. Hackers employ various techniques to jailbreak LLMs, ranging from simple synonym substitutions to more sopheborated attacks that exploit the models' context windows.
5. Despite reassurances from companies like Meta and OpenAI about their ongoing efforts to improve security measures, experts warn of increasing risks as AI systems become more interconnected with existing technology and devices.

**Conclusion:**

The rapid development and deployment of large language models have led to a global effort by hackers and researchers to identify vulnerabilities within these systems. As regulators worldwide take steps to address potential dangers, companies continue to invest in machine learning security startups while working on improving their AI systems' defenses against exploits and adversarial behavior.

[Source Link](https://ft.com/content/14a2c98b-c8d5-4e5b-a7b0-30f0a05ec432)

