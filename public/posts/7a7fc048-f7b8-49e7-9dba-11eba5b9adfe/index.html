<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>领先科技公司签署人工智能安全誓言 | HeadlineCapsule</title>
<meta name="keywords" content="">
<meta name="description" content="Artificial intelligence developers agree to assess when risks become ‘intolerable’ ahead of summit in Seoul">
<meta name="author" content="ft">
<link rel="canonical" href="https://headlinecapsule.github.io/posts/7a7fc048-f7b8-49e7-9dba-11eba5b9adfe/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://headlinecapsule.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://headlinecapsule.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://headlinecapsule.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://headlinecapsule.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://headlinecapsule.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="领先科技公司签署人工智能安全誓言" />
<meta property="og:description" content="Artificial intelligence developers agree to assess when risks become ‘intolerable’ ahead of summit in Seoul" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://headlinecapsule.github.io/posts/7a7fc048-f7b8-49e7-9dba-11eba5b9adfe/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-21T12:15:14+00:00" />
<meta property="article:modified_time" content="2024-05-21T12:15:14+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="领先科技公司签署人工智能安全誓言"/>
<meta name="twitter:description" content="Artificial intelligence developers agree to assess when risks become ‘intolerable’ ahead of summit in Seoul"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://headlinecapsule.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "领先科技公司签署人工智能安全誓言",
      "item": "https://headlinecapsule.github.io/posts/7a7fc048-f7b8-49e7-9dba-11eba5b9adfe/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "领先科技公司签署人工智能安全誓言",
  "name": "领先科技公司签署人工智能安全誓言",
  "description": "Artificial intelligence developers agree to assess when risks become ‘intolerable’ ahead of summit in Seoul",
  "keywords": [
    
  ],
  "articleBody": "原文链接\n领先科技公司签署人工智能安全誓言\n科技巨头签署自愿承诺以确保人工智能安全 摘要： 在首尔举行的一场全球 “人工智能峰会” 之前，领先科技公司 自愿达成了一项新的承诺，重点关注 “人工智能安全” 。 签约公司包括亚马逊、谷歌、Meta、微软、OpenAI、xAI 和智普人工智能等。 这些公司将发布评估和管理与其先进的 “前沿\"人工智能模型 相关的风险框架，并承诺在无法解决严重风险时停止开发或部署。 这是在去年布莱奇利宣言中讨论的 人工智能安全 的进一步延伸，也是这些公司去年在白宫做出的自愿承诺的延续。 虽然执行细节尚不清楚，但这些公司同意在实施时保持透明，同时平衡商业敏感性。 问题与答案： 科技公司签署的自愿承诺的主题是什么？\n这些承诺的主要焦点是建立实践和标准，以确保先进人工智能系统的安全开发和部署。其中包括风险评估、透明度、问责制，以及界定何为 “不可接受\"的风险 。 就人工智能安全而言，这一公告为何意义重大？\n这代表了人工智能领域的一些主要参与者协调一致的自律行为，也是他们为确立负责任的人工智能开发标准所做努力。这表明他们认识到了先进人工智能系统带来的潜在风险，并愿意主动采取措施。 这些承诺如何拓展之前关于人工智能安全的讨论？\n这些承诺是在去年 11 月由英国首相里希·苏纳克主持的首届人工智能安全峰会上提出的布莱奇利宣言的延伸。同时，也呼应了其中一些公司去年在白宫做出的自愿承诺，表明了在人工智能安全实践方面日益增长的共识和动力。 这些公司将采取什么具体行动来确保人工智能安全？\n这些公司将发布框架，概述其风险评估流程，包括在部署前和训练期间如何识别与他们的前沿人工智能模型相关的风险。他们还将设定不可接受的风险阈值并详细说明缓解策略。 这些承诺将如何被执行，又有哪些潜在挑战？\n执行是关键所在。虽然公司们同意在履行承诺方面保持透明，但在透明和保护敏感商业信息之间存在着微妙平衡。英国政府的立场是现在立法为时过早，强调首先需要更好地了解风险。 这对于人工智能行业和社会整体可能意味着什么？\n这些自愿承诺有可能成为全球人工智能安全标准的先例，更广泛地释放这项变革 Tech Giants Sign Voluntary Pledges for AI Safety Summary: Ahead of a global AI summit in Seoul, leading tech companies have agreed to a new set of voluntary commitments focused on AI safety. Amazon, Google, Meta, Microsoft, OpenAI, xAI, and Zhipu AI are among the signatories. The companies will publish frameworks for assessing and managing risks associated with their advanced “frontier” AI models, with a commitment to halt development or deployment if severe risks cannot be addressed. This builds on previous discussions on AI safety at the Bletchley Declaration and voluntary commitments made by some of these companies last year. While the specifics of enforcement remain unclear, the companies have agreed to provide transparency on their implementation, balancing it with commercial sensitivities. Questions \u0026 Answers: What are the key themes of the pledges signed by these tech companies?\nThe primary focus of these pledges is on establishing practices and standards for ensuring the safe development and deployment of advanced AI systems. This includes risk assessment, transparency, accountability, and establishing thresholds for what constitutes “intolerable” risks. Why is this announcement significant in the context of AI safety?\nIt represents a coordinated effort by some of the biggest players in AI to self-regulate and establish standards for responsible AI development. This demonstrates a recognition of the potential risks associated with advanced AI systems and a willingness to take proactive measures. How do these pledges build upon previous discussions on AI safety?\nThese pledges expand upon the Bletchley Declaration, made at the inaugural AI Safety Summit hosted by UK Prime Minister Rishi Sunak in November. They also echo voluntary commitments made by some of these companies at the White House last year, indicating a growing consensus and momentum around AI safety practices. What specific actions will the companies take to ensure AI safety?\nThe companies will publish frameworks outlining their risk assessment processes, including how they identify risks associated with their frontier AI models before deployment and during training. They will also set thresholds for what constitutes intolerable risks and detail mitigation strategies. How will these pledges be enforced, and what are the potential challenges?\nEnforcement remains a key question. While the companies have agreed to provide transparency on their implementation of the pledges, there is a delicate balance between this transparency and protecting sensitive commercial information. The UK government’s stance is that it is too early for legislation, emphasizing the need to first understand the risks better. What are the potential implications for the AI industry and society as a whole?\nThese pledges have the potential to set a precedent for global standards on AI safety, unlocking the benefits of this transformative technology more broadly. They demonstrate a responsible approach by industry leaders that could help build public trust in AI while also laying the foundation for future regulation. Are there any dissenting opinions or concerns raised about these pledges?\nWhile generally well-received, some may question the voluntary nature of these commitments and how they will be enforced. The balance between transparency and commercial sensitivities might also prove challenging in practice. Additionally, as AI safety is a rapidly evolving field, keeping regulations and standards up to date is crucial, as noted by UK Science Secretary Michelle Donelan. What are the next steps following these pledges?\nThe companies will now work on implementing their risk assessment frameworks and providing transparency on their progress. Meanwhile, governments will continue discussions at the AI summit in Seoul, with participation from a diverse range of countries including China. This report provides an overview of the key developments, themes, and potential implications arising from these voluntary pledges for AI safety. By signing these agreements, tech giants are taking a proactive approach to managing risks associated with advanced AI systems, which could shape the future development and public perception of this\nSource Link\n",
  "wordCount" : "670",
  "inLanguage": "en",
  "datePublished": "2024-05-21T12:15:14.729Z",
  "dateModified": "2024-05-21T12:15:14.729Z",
  "author":{
    "@type": "Person",
    "name": "ft"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://headlinecapsule.github.io/posts/7a7fc048-f7b8-49e7-9dba-11eba5b9adfe/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "HeadlineCapsule",
    "logo": {
      "@type": "ImageObject",
      "url": "https://headlinecapsule.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://headlinecapsule.github.io" accesskey="h" title="HeadlineCapsule (Alt + H)">HeadlineCapsule</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      领先科技公司签署人工智能安全誓言
    </h1>
    <div class="post-description">
      Artificial intelligence developers agree to assess when risks become ‘intolerable’ ahead of summit in Seoul
    </div>
    <div class="post-meta"><span title='2024-05-21 12:15:14.729 +0000 UTC'>May 21, 2024</span>&nbsp;·&nbsp;ft

</div>
  </header> 
  <div class="post-content"><p><a href="https://ft.com/content/7a7fc048-f7b8-49e7-9dba-11eba5b9adfe">原文链接</a></p>
<p>领先科技公司签署人工智能安全誓言</p>
<h1 id="科技巨头签署自愿承诺以确保人工智能安全"><strong>科技巨头签署自愿承诺以确保人工智能安全</strong><a hidden class="anchor" aria-hidden="true" href="#科技巨头签署自愿承诺以确保人工智能安全">#</a></h1>
<h2 id="摘要">摘要：<a hidden class="anchor" aria-hidden="true" href="#摘要">#</a></h2>
<ul>
<li>在首尔举行的一场全球 <strong>&ldquo;人工智能峰会&rdquo;</strong> 之前，<strong>领先科技公司</strong> 自愿达成了一项新的承诺，重点关注 <strong>&ldquo;人工智能安全&rdquo;</strong> 。</li>
<li>签约公司包括亚马逊、谷歌、Meta、微软、OpenAI、xAI 和智普人工智能等。</li>
<li>这些公司将发布评估和管理与其先进的 <strong>&ldquo;前沿&quot;人工智能模型</strong> 相关的风险框架，并承诺在无法解决严重风险时停止开发或部署。</li>
<li>这是在去年布莱奇利宣言中讨论的 <strong>人工智能安全</strong> 的进一步延伸，也是这些公司去年在白宫做出的自愿承诺的延续。</li>
<li>虽然执行细节尚不清楚，但这些公司同意在实施时保持透明，同时平衡商业敏感性。</li>
</ul>
<h2 id="问题与答案">问题与答案：<a hidden class="anchor" aria-hidden="true" href="#问题与答案">#</a></h2>
<ol>
<li>
<p><strong>科技公司签署的自愿承诺的主题是什么？</strong></p>
<ul>
<li>这些承诺的主要焦点是建立实践和标准，以确保先进人工智能系统的安全开发和部署。其中包括风险评估、透明度、问责制，以及界定何为 <strong>&ldquo;不可接受&quot;的风险</strong> 。</li>
</ul>
</li>
<li>
<p><strong>就人工智能安全而言，这一公告为何意义重大？</strong></p>
<ul>
<li>这代表了人工智能领域的一些主要参与者协调一致的自律行为，也是他们为确立负责任的人工智能开发标准所做努力。这表明他们认识到了先进人工智能系统带来的潜在风险，并愿意主动采取措施。</li>
</ul>
</li>
<li>
<p><strong>这些承诺如何拓展之前关于人工智能安全的讨论？</strong></p>
<ul>
<li>这些承诺是在去年 11 月由英国首相里希·苏纳克主持的首届人工智能安全峰会上提出的布莱奇利宣言的延伸。同时，也呼应了其中一些公司去年在白宫做出的自愿承诺，表明了在人工智能安全实践方面日益增长的共识和动力。</li>
</ul>
</li>
<li>
<p><strong>这些公司将采取什么具体行动来确保人工智能安全？</strong></p>
<ul>
<li>这些公司将发布框架，概述其风险评估流程，包括在部署前和训练期间如何识别与他们的前沿人工智能模型相关的风险。他们还将设定不可接受的风险阈值并详细说明缓解策略。</li>
</ul>
</li>
<li>
<p><strong>这些承诺将如何被执行，又有哪些潜在挑战？</strong></p>
<ul>
<li>执行是关键所在。虽然公司们同意在履行承诺方面保持透明，但在透明和保护敏感商业信息之间存在着微妙平衡。英国政府的立场是现在立法为时过早，强调首先需要更好地了解风险。</li>
</ul>
</li>
<li>
<p><strong>这对于人工智能行业和社会整体可能意味着什么？</strong></p>
<ul>
<li>这些自愿承诺有可能成为全球人工智能安全标准的先例，更广泛地释放这项变革</li>
</ul>
</li>
</ol>
<hr>
<h1 id="tech-giants-sign-voluntary-pledges-for-ai-safety"><strong>Tech Giants Sign Voluntary Pledges for AI Safety</strong><a hidden class="anchor" aria-hidden="true" href="#tech-giants-sign-voluntary-pledges-for-ai-safety">#</a></h1>
<h2 id="summary">Summary:<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<ul>
<li>Ahead of a global <strong>AI summit in Seoul</strong>, <strong>leading tech companies</strong> have agreed to a new set of voluntary commitments focused on <strong>AI safety</strong>.</li>
<li>Amazon, Google, Meta, Microsoft, OpenAI, xAI, and Zhipu AI are among the signatories.</li>
<li>The companies will publish frameworks for assessing and managing risks associated with their advanced <strong>&ldquo;frontier&rdquo; AI models</strong>, with a commitment to halt development or deployment if severe risks cannot be addressed.</li>
<li>This builds on previous discussions on <strong>AI safety</strong> at the Bletchley Declaration and voluntary commitments made by some of these companies last year.</li>
<li>While the specifics of enforcement remain unclear, the companies have agreed to provide transparency on their implementation, balancing it with commercial sensitivities.</li>
</ul>
<h2 id="questions--answers">Questions &amp; Answers:<a hidden class="anchor" aria-hidden="true" href="#questions--answers">#</a></h2>
<ol>
<li>
<p><strong>What are the key themes of the pledges signed by these tech companies?</strong></p>
<ul>
<li>The primary focus of these pledges is on establishing practices and standards for ensuring the safe development and deployment of advanced AI systems. This includes risk assessment, transparency, accountability, and establishing thresholds for what constitutes &ldquo;intolerable&rdquo; risks.</li>
</ul>
</li>
<li>
<p><strong>Why is this announcement significant in the context of AI safety?</strong></p>
<ul>
<li>It represents a coordinated effort by some of the biggest players in AI to self-regulate and establish standards for responsible AI development. This demonstrates a recognition of the potential risks associated with advanced AI systems and a willingness to take proactive measures.</li>
</ul>
</li>
<li>
<p><strong>How do these pledges build upon previous discussions on AI safety?</strong></p>
<ul>
<li>These pledges expand upon the Bletchley Declaration, made at the inaugural AI Safety Summit hosted by UK Prime Minister Rishi Sunak in November. They also echo voluntary commitments made by some of these companies at the White House last year, indicating a growing consensus and momentum around AI safety practices.</li>
</ul>
</li>
<li>
<p><strong>What specific actions will the companies take to ensure AI safety?</strong></p>
<ul>
<li>The companies will publish frameworks outlining their risk assessment processes, including how they identify risks associated with their frontier AI models before deployment and during training. They will also set thresholds for what constitutes intolerable risks and detail mitigation strategies.</li>
</ul>
</li>
<li>
<p><strong>How will these pledges be enforced, and what are the potential challenges?</strong></p>
<ul>
<li>Enforcement remains a key question. While the companies have agreed to provide transparency on their implementation of the pledges, there is a delicate balance between this transparency and protecting sensitive commercial information. The UK government&rsquo;s stance is that it is too early for legislation, emphasizing the need to first understand the risks better.</li>
</ul>
</li>
<li>
<p><strong>What are the potential implications for the AI industry and society as a whole?</strong></p>
<ul>
<li>These pledges have the potential to set a precedent for global standards on AI safety, unlocking the benefits of this transformative technology more broadly. They demonstrate a responsible approach by industry leaders that could help build public trust in AI while also laying the foundation for future regulation.</li>
</ul>
</li>
<li>
<p><strong>Are there any dissenting opinions or concerns raised about these pledges?</strong></p>
<ul>
<li>While generally well-received, some may question the voluntary nature of these commitments and how they will be enforced. The balance between transparency and commercial sensitivities might also prove challenging in practice. Additionally, as AI safety is a rapidly evolving field, keeping regulations and standards up to date is crucial, as noted by UK Science Secretary Michelle Donelan.</li>
</ul>
</li>
<li>
<p><strong>What are the next steps following these pledges?</strong></p>
<ul>
<li>The companies will now work on implementing their risk assessment frameworks and providing transparency on their progress. Meanwhile, governments will continue discussions at the AI summit in Seoul, with participation from a diverse range of countries including China.</li>
</ul>
</li>
</ol>
<p>This report provides an overview of the key developments, themes, and potential implications arising from these voluntary pledges for <strong>AI safety</strong>. By signing these agreements, tech giants are taking a proactive approach to managing risks associated with advanced AI systems, which could shape the future development and public perception of this</p>
<p><a href="https://ft.com/content/7a7fc048-f7b8-49e7-9dba-11eba5b9adfe">Source Link</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://headlinecapsule.github.io">HeadlineCapsule</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
